from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Type
from dataclasses import dataclass
from pathlib import Path
import mimetypes
from loguru import logger
from PIL import Image
import pytesseract
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
import pandas
import ezdxf
import time
import os
import io


@dataclass
class ParserResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    success: bool
    text: str
    error_message: str
    metadata: Dict[str, Any]
    file_path: str
    
    def is_success(self) -> bool:
        return self.success
    
    def get_text(self) -> str:
        return self.text
    
    def get_error(self) -> str:
        return self.error_message
    
    def get_metadata(self) -> Dict:
        return self.metadata


class BaseParser(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–∞—Ä—Å–µ—Ä–∞"""
    
    @abstractmethod
    def parse(self, file_path: str, **params) -> ParserResult:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞"""
        pass
    
    @abstractmethod
    def get_supported_extensions(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        pass
    
    def validate_file(self, file_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞"""
        if not Path(file_path).exists():
            return False
        
        ext = Path(file_path).suffix.lower()
        return ext in self.get_supported_extensions()


class FileValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä —Ñ–∞–π–ª–æ–≤"""
    
    @staticmethod
    def validate_file_exists(file_path: str) -> bool:
        return Path(file_path).exists()
    
    @staticmethod
    def validate_file_size(file_path: str, max_size: int) -> bool:
        if not Path(file_path).exists():
            return False
        return Path(file_path).stat().st_size <= max_size
    
    @staticmethod
    def validate_file_type(file_path: str, expected_extensions: List[str]) -> bool:
        ext = Path(file_path).suffix.lower()
        return ext in expected_extensions
    
    @staticmethod
    def get_file_mime_type(file_path: str) -> str:
        mime_type, _ = mimetypes.guess_type(file_path)
        return mime_type or 'application/octet-stream'


class PDFParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, use_ocr: bool = True, extract_images: bool = False, 
                 ocr_language: str = "rus+eng"):
        self.use_ocr = use_ocr
        self.extract_images = extract_images
        self.ocr_language = ocr_language
    
    def parse(self, file_path: str, **params) -> ParserResult:

        """ –ü–∞—Ä—Å–∏–Ω–≥ PDF —Ñ–∞–π–ª–∞ """

        start_time = time.time()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            use_ocr = params.get('use_ocr', self.use_ocr)
            ocr_language = params.get('ocr_language', self.ocr_language)
            pages = params.get('pages', None)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
            if not os.path.exists(file_path):
                return ParserResult(
                    success=False,
                    text="",
                    error_message=f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}",
                    metadata={},
                    file_path=file_path
                )
            
            # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞
            if use_ocr:
                import pytesseract
                from PIL import Image
                import pymupdf as fitz
                text, metadata = self._extract_with_ocr(file_path, pages, ocr_language)
                method = "ocr"
            else:
                text, metadata = self._extract_with_pymupdf(file_path, pages)
                method = "text_extraction"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            pdf_metadata = self._extract_pdf_metadata(file_path)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            final_metadata = {
                'parser': 'PDFParser',
                'method': method,
                'use_ocr': use_ocr,
                'processing_time_sec': round(time.time() - start_time, 2),
                'text_length': len(text),
                **metadata,
                **pdf_metadata
            }
            
            return ParserResult(
                success=True,
                text=text,
                error_message="",
                metadata=final_metadata,
                file_path=file_path
            )
            
        except Exception as e:
            logger.error(f"PDF parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _extract_with_pymupdf(self, file_path: str, pages: List[int] = None) -> tuple:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é PyMuPDF (fitz) - –°–ê–ú–´–ô –ë–´–°–¢–†–´–ô"""
        import pymupdf as fitz
        
        text = ""
        metadata = {}
        
        with fitz.open(file_path) as doc:
            metadata['total_pages'] = len(doc)
            metadata['is_encrypted'] = doc.is_encrypted
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if pages:
                page_indices = [p-1 for p in pages if 1 <= p <= len(doc)]
            else:
                page_indices = range(len(doc))
            
            for page_num in page_indices:
                page = doc[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    text += f"--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n{page_text}\n"
            
            metadata['pages_processed'] = len(page_indices)
        
        return text, metadata
    
    def _extract_with_ocr(self, file_path: str, pages: List[int] = None, language: str = "rus+eng") -> tuple:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é OCR"""
        import pymupdf as fitz
        import pytesseract
        from PIL import Image
        
        text = ""
        metadata = {
            'ocr_language': language,
            'ocr_engine': 'tesseract'
        }
        
        with fitz.open(file_path) as doc:
            metadata['total_pages'] = len(doc)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if pages:
                page_indices = [p-1 for p in pages if 1 <= p <= len(doc)]
            else:
                page_indices = range(len(doc))
            
            ocr_pages_count = 0
            
            for page_num in page_indices:
                page = doc[page_num]
                
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                page_text = page.get_text()
                if page_text.strip():
                    text += f"--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} (–¢–ï–ö–°–¢) ---\n{page_text}\n"
                else:
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR
                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
                    img_data = pix.tobytes("png")
                    
                    with Image.open(io.BytesIO(img_data)) as img:
                        ocr_text = pytesseract.image_to_string(img, lang=language)
                        if ocr_text.strip():
                            text += f"--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} (OCR) ---\n{ocr_text}\n"
                            ocr_pages_count += 1
            
            metadata['pages_processed'] = len(page_indices)
            metadata['ocr_pages'] = ocr_pages_count
        
        return text, metadata
    
    def _extract_pdf_metadata(self, file_path: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö PDF"""
        import pymupdf as fitz
        
        metadata = {}
        
        try:
            with fitz.open(file_path) as doc:
                pdf_metadata = doc.metadata
                metadata.update({
                    'author': pdf_metadata.get('author', ''),
                    'title': pdf_metadata.get('title', ''),
                    'subject': pdf_metadata.get('subject', ''),
                    'keywords': pdf_metadata.get('keywords', ''),
                    'creator': pdf_metadata.get('creator', ''),
                    'producer': pdf_metadata.get('producer', ''),
                    'creation_date': pdf_metadata.get('creationDate', ''),
                    'modification_date': pdf_metadata.get('modDate', ''),
                })
        except:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
        file_stats = os.stat(file_path)
        metadata.update({
            'file_size_bytes': file_stats.st_size,
        })
        
        return metadata
    
    def get_supported_extensions(self) -> List[str]:
        return ['.pdf']


class DOCParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è .doc —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    
    def parse(self, file_path: str, **params) -> ParserResult:
        try:
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext != '.doc':
                return ParserResult(
                    success=False,
                    text="",
                    error_message="DOCParser supports only .doc files",
                    metadata={},
                    file_path=file_path
                )
            
            # –ü—Ä–æ–±—É–µ–º –º–µ—Ç–æ–¥—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            return self._try_all_doc_methods(file_path)
            
        except Exception as e:
            logger.error(f"DOC parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _try_all_doc_methods(self, file_path: str) -> ParserResult:
        """–ü—Ä–æ–±—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ .doc"""
        
        methods = [
            self._parse_with_olefile,      # –ú–µ—Ç–æ–¥ 1: OLE —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            self._parse_with_antiword,     # –ú–µ—Ç–æ–¥ 2: antiword
            self._parse_with_strings,      # –ú–µ—Ç–æ–¥ 3: strings command
            self._parse_smart_binary,      # –ú–µ—Ç–æ–¥ 4: –£–º–Ω–æ–µ –±–∏–Ω–∞—Ä–Ω–æ–µ —á—Ç–µ–Ω–∏–µ
        ]
        
        for method in methods:
            result = method(file_path)
            if result.success:
                logger.info(f"DOC parsed successfully with {result.metadata.get('method')}")
                return result
        
        return ParserResult(
            success=False,
            text="",
            error_message="All .doc parsing methods failed. File may be corrupted, encrypted, or in unsupported format.",
            metadata={},
            file_path=file_path
        )
    
    def _parse_with_olefile(self, file_path: str) -> ParserResult:
        """–ü–∞—Ä—Å–∏–Ω–≥ .doc —á–µ—Ä–µ–∑ OLE —Å—Ç—Ä—É–∫—Ç—É—Ä—É (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –¥–ª—è —Å—Ç–∞—Ä—ã—Ö Word)"""
        try:
            import olefile
            
            if not olefile.isOleFile(file_path):
                return ParserResult(success=False, text="", error_message="Not a valid OLE file", metadata={}, file_path=file_path)
            
            ole = olefile.OleFileIO(file_path)
            text_parts = []
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            streams = ['WordDocument', 'Table', 'Data', 'SummaryInformation']
            
            for stream in streams:
                if ole.exists(stream):
                    try:
                        data = ole.openstream(stream).read()
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö
                        stream_text = self._extract_text_from_bytes(data)
                        if stream_text:
                            text_parts.append(stream_text)
                    except:
                        continue
            
            ole.close()
            
            if text_parts:
                text = '\n'.join(text_parts)
                return ParserResult(
                    success=True,
                    text=text,
                    error_message="",
                    metadata={'method': 'olefile', 'parser': 'DOCParser'},
                    file_path=file_path
                )
            else:
                return ParserResult(success=False, text="", error_message="No text found in OLE streams", metadata={}, file_path=file_path)
                
        except ImportError:
            return ParserResult(success=False, text="", error_message="olefile not installed", metadata={}, file_path=file_path)
        except Exception as e:
            return ParserResult(success=False, text="", error_message=str(e), metadata={}, file_path=file_path)
    
    def _parse_with_antiword(self, file_path: str) -> ParserResult:
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ antiword (—Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ antiword)"""
        try:
            import subprocess
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å antiword
            result = subprocess.run(['antiword', '-v'], capture_output=True, text=True)
            if result.returncode != 0:
                return ParserResult(success=False, text="", error_message="antiword not available", metadata={}, file_path=file_path)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º antiword
            result = subprocess.run(
                ['antiword', '-m', 'UTF-8.txt', file_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore',
                timeout=30
            )
            
            if result.returncode == 0 and result.stdout.strip():
                return ParserResult(
                    success=True,
                    text=result.stdout,
                    error_message="",
                    metadata={'method': 'antiword', 'parser': 'DOCParser'},
                    file_path=file_path
                )
            else:
                return ParserResult(success=False, text="", error_message="antiword failed", metadata={}, file_path=file_path)
                
        except Exception as e:
            return ParserResult(success=False, text="", error_message=str(e), metadata={}, file_path=file_path)
    
    def _parse_with_strings(self, file_path: str) -> ParserResult:
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ strings –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            import subprocess
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º strings –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            result = subprocess.run(
                ['strings', '-n', '10', file_path],  # –ú–∏–Ω–∏–º—É–º 10 —Å–∏–º–≤–æ–ª–æ–≤
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore',
                timeout=30
            )
            
            if result.returncode == 0 and result.stdout.strip():
                lines = [line.strip() for line in result.stdout.split('\n') if len(line.strip()) > 10]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å —Ä—É—Å—Å–∫–∏–º–∏ –±—É–∫–≤–∞–º–∏
                russian_lines = []
                for line in lines:
                    russian_count = sum(1 for c in line if '–∞' <= c <= '—è' or '–ê' <= c <= '–Ø')
                    if russian_count > len(line) * 0.2:  # –•–æ—Ç—è –±—ã 20% —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤
                        russian_lines.append(line)
                
                if russian_lines:
                    text = '\n'.join(russian_lines)
                    return ParserResult(
                        success=True,
                        text=text,
                        error_message="",
                        metadata={'method': 'strings', 'parser': 'DOCParser'},
                        file_path=file_path
                    )
            
            return ParserResult(success=False, text="", error_message="No readable text found with strings", metadata={}, file_path=file_path)
            
        except Exception as e:
            return ParserResult(success=False, text="", error_message=str(e), metadata={}, file_path=file_path)
    
    def _parse_smart_binary(self, file_path: str) -> ParserResult:
        """–£–º–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –ø–æ–∏—Å–∫–æ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ –≤ –±–∏–Ω–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            text_blocks = self._find_text_blocks(content)
            
            if text_blocks:
                text = '\n'.join(text_blocks)
                return ParserResult(
                    success=True,
                    text=text,
                    error_message="",
                    metadata={'method': 'smart_binary', 'parser': 'DOCParser', 'blocks_found': len(text_blocks)},
                    file_path=file_path
                )
            else:
                return ParserResult(success=False, text="", error_message="No text blocks found", metadata={}, file_path=file_path)
                
        except Exception as e:
            return ParserResult(success=False, text="", error_message=str(e), metadata={}, file_path=file_path)
    
    def _find_text_blocks(self, content: bytes) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ –≤ –±–∏–Ω–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        text_blocks = []
        
        # –ö–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏
        encodings = ['utf-16-le', 'utf-16-be', 'cp1251', 'cp866', 'koi8-r']
        
        for encoding in encodings:
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å —Ñ–∞–π–ª
                decoded = content.decode(encoding, errors='ignore')
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                lines = []
                for line in decoded.split('\n'):
                    line = line.strip()
                    if 10 <= len(line) <= 1000:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ–∫—Å—Ç (–±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è)
                        text_chars = sum(1 for c in line if c.isalnum() or c in ' .,!?;-')
                        if text_chars > len(line) * 0.6:  # –•–æ—Ç—è –±—ã 60% —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                            lines.append(line)
                
                if lines:
                    text_blocks.extend(lines)
                    break
                    
            except Exception:
                continue
        
        return text_blocks
    
    def _extract_text_from_bytes(self, data: bytes) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–π—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        text = ""
        
        for encoding in ['utf-16-le', 'utf-8', 'cp1251', 'cp866']:
            try:
                decoded = data.decode(encoding, errors='ignore')
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –±—É–∫–≤–∞–º–∏
                lines = [line.strip() for line in decoded.split('\n') if len(line.strip()) > 5 and any(c.isalpha() for c in line)]
                if lines:
                    text = '\n'.join(lines)
                    break
            except:
                continue
        
        return text
    
    def get_supported_extensions(self) -> List[str]:
        return ['.doc']


class DOCXParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –¢–û–õ–¨–ö–û –¥–ª—è DOCX —Ñ–∞–π–ª–æ–≤"""
    
    def parse(self, file_path: str, **params) -> ParserResult:
        try:
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext != '.docx':
                return ParserResult(
                    success=False,
                    text="",
                    error_message=f"DOCXParser supports only .docx files. Use DOCParser for .doc files.",
                    metadata={},
                    file_path=file_path
                )
            
            from docx import Document
            doc = Document(file_path)
            text = self._parse_document_structure(doc)
            
            metadata = {
                'parser': 'DOCXParser',
                'paragraphs_count': len(doc.paragraphs),
                'tables_count': len(doc.tables)
            }
            
            return ParserResult(
                success=True,
                text=text,
                error_message="",
                metadata=metadata,
                file_path=file_path
            )
        except Exception as e:
            logger.error(f"DOCX parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _parse_document_structure(self, doc) -> str:
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        for table in doc.tables:
            for row in table.rows:
                row_text = [cell.text for cell in row.cells if cell.text.strip()]
                if row_text:
                    text += " | ".join(row_text) + "\n"
        
        return text
    
    def get_supported_extensions(self) -> List[str]:
        return ['.docx']


class XLSXParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä Excel –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, read_formulas: bool = False, sheet_names: Optional[List[str]] = None):
        self.read_formulas = read_formulas
        self.sheet_names = sheet_names
    
    def parse(self, file_path: str, **params) -> ParserResult:
        try:
            import pandas as pd
            
            if self.sheet_names:
                excel_file = pd.read_excel(file_path, sheet_name=self.sheet_names)
            else:
                excel_file = pd.read_excel(file_path, sheet_name=None)
            
            text = self._parse_sheets(excel_file)
            
            metadata = {
                'parser': 'XLSXParser',
                'sheets_count': len(excel_file) if isinstance(excel_file, dict) else 1
            }
            
            return ParserResult(
                success=True,
                text=text,
                error_message="",
                metadata=metadata,
                file_path=file_path
            )
        except Exception as e:
            logger.error(f"XLSX parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _parse_sheets(self, excel_file) -> str:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–∏—Å—Ç–æ–≤ Excel"""
        text = ""
        
        if isinstance(excel_file, dict):
            for sheet_name, df in excel_file.items():
                text += f"\n--- –õ–∏—Å—Ç: {sheet_name} ---\n"
                text += df.to_string() + "\n"
        else:
            text = excel_file.to_string()
        
        return text
    
    def get_supported_extensions(self) -> List[str]:
        return ['.xlsx', '.xls']


class PlainTextParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, encodings: Optional[List[str]] = None):
        self.encodings = encodings or ['utf-8', 'cp1251', 'latin-1']
    
    def parse(self, file_path: str, **params) -> ParserResult:
        try:
            text = self._try_encodings(file_path)
            
            metadata = {
                'parser': 'PlainTextParser',
                'encoding': 'detected'
            }
            
            return ParserResult(
                success=True,
                text=text,
                error_message="",
                metadata=metadata,
                file_path=file_path
            )
        except Exception as e:
            logger.error(f"Text parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _try_encodings(self, file_path: str) -> str:
        """–ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º–∏"""
        for encoding in self.encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    
    def get_supported_extensions(self) -> List[str]:
        return ['.txt', '.log', '.csv', '.md']

class DWGParser(BaseParser):
    pass


class DXFParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä DXF —Ñ–∞–π–ª–æ–≤ (AutoCAD Drawing Exchange Format)"""
    
    def __init__(self, extract_metadata: bool = True, extract_blocks: bool = True,
                 search_in_blocks: bool = True, search_in_layouts: bool = True):
        self.extract_metadata = extract_metadata
        self.extract_blocks = extract_blocks
        self.search_in_blocks = search_in_blocks
        self.search_in_layouts = search_in_layouts
    
    def parse(self, file_path: str, **params) -> ParserResult:

        """ –ü–∞—Ä—Å–∏–Ω–≥ DXF —Ñ–∞–π–ª–∞ """
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            search_in_blocks = params.get('search_in_blocks', self.search_in_blocks)
            search_in_layouts = params.get('search_in_layouts', self.search_in_layouts)
            search_in_entities = params.get('search_in_entities', True)
            deep_search = params.get('deep_search', True)
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º DXF —Ñ–∞–π–ª
            doc = ezdxf.readfile(file_path)
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –≤ –†–ê–ó–ù–´–• –º–µ—Å—Ç–∞—Ö
            text_content = ""
            
            if search_in_entities:
                text_content += self._extract_from_entities(doc)
            
            if search_in_blocks:
                text_content += self._extract_from_blocks(doc, deep_search)
            
            if search_in_layouts:
                text_content += self._extract_from_layouts(doc)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = self._extract_metadata(doc) if self.extract_metadata else {}
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
            final_text = self._format_output(text_content, metadata, params)
            
            return ParserResult(
                success=True,
                text=final_text,
                error_message="",
                metadata=metadata,
                file_path=file_path
            )
            
        except Exception as e:
            return ParserResult(
                success=False,
                text="",
                error_message=f"DXF parsing error: {str(e)}",
                metadata={},
                file_path=file_path
            )
    
    def _extract_from_entities(self, doc) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ entities (–ø—Ä—è–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤)"""
        text_parts = []
        msp = doc.modelspace()
        
        # –°—á–µ—Ç—á–∏–∫–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
        counters = {'TEXT': 0, 'MTEXT': 0, 'ATTDEF': 0, 'ATTRIB': 0}
        
        # TEXT entities
        for text in msp.query('TEXT'):
            if text.dxf.text and text.dxf.text.strip():
                text_parts.append(f"TEXT: {text.dxf.text}")
                counters['TEXT'] += 1
        
        # MTEXT entities (–º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç)
        for mtext in msp.query('MTEXT'):
            if mtext.text and mtext.text.strip():
                text_parts.append(f"MTEXT: {mtext.text}")
                counters['MTEXT'] += 1
        
        # ATTDEF (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤)
        for attdef in msp.query('ATTDEF'):
            if attdef.dxf.tag and attdef.dxf.default_value:
                text_parts.append(f"ATTR_DEF: {attdef.dxf.tag} = {attdef.dxf.default_value}")
                counters['ATTDEF'] += 1
        
        # ATTRIB (–∞—Ç—Ä–∏–±—É—Ç—ã –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤)
        for attrib in msp.query('ATTRIB'):
            if attrib.dxf.text and attrib.dxf.text.strip():
                text_parts.append(f"ATTR: {attrib.dxf.text}")
                counters['ATTRIB'] += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if any(counters.values()):
            text_parts.append(f"\nüìä –ù–∞–π–¥–µ–Ω–æ –≤ entities: TEXT={counters['TEXT']}, MTEXT={counters['MTEXT']}, ATTDEF={counters['ATTDEF']}, ATTRIB={counters['ATTRIB']}")
        
        return "\n".join(text_parts) + "\n" if text_parts else ""
    
    def _extract_from_blocks(self, doc, deep_search: bool = True) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–ª–æ–∫–æ–≤"""
        text_parts = []
        total_blocks_searched = 0
        total_text_found = 0
        
        for block in doc.blocks:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –±–ª–æ–∫–∏
            if block.name.startswith('*'):
                continue
                
            total_blocks_searched += 1
            block_text_parts = []
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞
            for entity in block:
                entity_text = self._extract_text_from_entity(entity)
                if entity_text:
                    block_text_parts.append(f"  - {entity_text}")
                    total_text_found += 1
            
            # –ï—Å–ª–∏ –≤ –±–ª–æ–∫–µ –Ω–∞—à–ª–∏ —Ç–µ–∫—Å—Ç - –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if block_text_parts:
                text_parts.append(f"üî∑ –ë–õ–û–ö: {block.name}")
                text_parts.extend(block_text_parts)
                text_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–ª–æ–∫–∞–º
        if total_blocks_searched > 0:
            text_parts.append(f"üìä –ü–æ–∏—Å–∫ –≤ –±–ª–æ–∫–∞—Ö: –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {total_blocks_searched} –±–ª–æ–∫–æ–≤, –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–∞ –≤ {total_text_found} –º–µ—Å—Ç–∞—Ö")
        
        return "\n".join(text_parts) + "\n" if text_parts else ""
    
    def _extract_from_layouts(self, doc) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ layout'–æ–≤ (Paper Space)"""
        text_parts = []
        
        for layout in doc.layouts:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º Model Space (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏)
            if layout.name == 'Model':
                continue
                
            layout_text_parts = []
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –≤ layout'–µ
            for entity in layout:
                entity_text = self._extract_text_from_entity(entity)
                if entity_text:
                    layout_text_parts.append(f"  - {entity_text}")
            
            # –ï—Å–ª–∏ –≤ layout'–µ –Ω–∞—à–ª–∏ —Ç–µ–∫—Å—Ç
            if layout_text_parts:
                text_parts.append(f"üìÑ LAYOUT: {layout.name}")
                text_parts.extend(layout_text_parts)
                text_parts.append("")
        
        return "\n".join(text_parts) + "\n" if text_parts else ""
    
    def _extract_text_from_entity(self, entity) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ entity"""
        try:
            if entity.dxftype() == 'TEXT' and entity.dxf.text and entity.dxf.text.strip():
                return f"TEXT: {entity.dxf.text}"
            elif entity.dxftype() == 'MTEXT' and entity.text and entity.text.strip():
                return f"MTEXT: {entity.text}"
            elif entity.dxftype() == 'ATTDEF' and entity.dxf.tag and entity.dxf.default_value:
                return f"ATTR_DEF: {entity.dxf.tag} = {entity.dxf.default_value}"
            elif entity.dxftype() == 'ATTRIB' and entity.dxf.text and entity.dxf.text.strip():
                return f"ATTR: {entity.dxf.text}"
        except:
            pass
        return ""
    
    def _extract_metadata(self, doc) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö DXF —Ñ–∞–π–ª–∞"""
        metadata = {
            'dxf_version': str(doc.dxfversion),
            'layers_count': len(doc.layers),
            'blocks_count': len(doc.blocks),
            'entities_count': len(doc.modelspace()),
            'layouts_count': len(doc.layouts) - 1,  # -1 –ø–æ—Ç–æ–º—É —á—Ç–æ Model —Ç–æ–∂–µ layout
            'file_units': str(doc.header.get('$INSUNITS', 'Unknown')),
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –æ–±—ä–µ–∫—Ç–æ–≤
        msp = doc.modelspace()
        entity_stats = {
            'TEXT': len(msp.query('TEXT')),
            'MTEXT': len(msp.query('MTEXT')),
            'ATTDEF': len(msp.query('ATTDEF')),
            'ATTRIB': len(msp.query('ATTRIB')),
            'INSERT': len(msp.query('INSERT')),  # –í—Å—Ç–∞–≤–∫–∏ –±–ª–æ–∫–æ–≤
        }
        metadata['entity_statistics'] = entity_stats
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–æ—è—Ö
        layers_info = []
        for layer in doc.layers:
            layers_info.append({
                'name': layer.dxf.name,
                'color': layer.dxf.color,
                'is_off': layer.is_off(),
            })
        metadata['layers'] = layers_info
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–ª–æ–∫–∞—Ö
        blocks_info = []
        for block in doc.blocks:
            if not block.name.startswith('*'):
                blocks_info.append({
                    'name': block.name,
                    'entities_count': len(block),
                })
        metadata['blocks'] = blocks_info
        
        return metadata
    
    def _format_output(self, text_content: str, metadata: Dict, params: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        output_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if params.get('include_metadata', True):
            output_parts.append("=== –ú–ï–¢–ê–î–ê–ù–ù–´–ï DXF ===")
            output_parts.append(f"–í–µ—Ä—Å–∏—è DXF: {metadata.get('dxf_version', 'Unknown')}")
            output_parts.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {metadata.get('layers_count', 0)}")
            output_parts.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤: {metadata.get('blocks_count', 0)}")
            output_parts.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ Model: {metadata.get('entities_count', 0)}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤
            stats = metadata.get('entity_statistics', {})
            output_parts.append(f"üìä –û–±—ä–µ–∫—Ç—ã: TEXT={stats.get('TEXT', 0)}, MTEXT={stats.get('MTEXT', 0)}, ATTDEF={stats.get('ATTDEF', 0)}, ATTRIB={stats.get('ATTRIB', 0)}")
            output_parts.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if text_content.strip():
            output_parts.append("=== –¢–ï–ö–°–¢–û–í–û–ï –°–û–î–ï–†–ñ–ò–ú–û–ï ===")
            output_parts.append(text_content)
        else:
            output_parts.append("=== –¢–ï–ö–°–¢ –ù–ï –ù–ê–ô–î–ï–ù ===")
            output_parts.append("–¢–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤:")
            output_parts.append("- Model Space entities")
            output_parts.append("- –ë–ª–æ–∫–∞—Ö (blocks)")
            output_parts.append("- Layout'–∞—Ö (Paper Space)")
            output_parts.append("")
            output_parts.append("üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            output_parts.append("- –¢–µ–∫—Å—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–∞—Ö (INSERT)")
            output_parts.append("- –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –≥–µ–æ–º–µ—Ç—Ä–∏—é –±–µ–∑ —Ç–µ–∫—Å—Ç–∞")
            output_parts.append("- –¢–µ–∫—Å—Ç –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö")
        
        return "\n".join(output_parts)
    
    def get_supported_extensions(self) -> List[str]:
        return ['.dxf']

class ImageOCRParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å OCR"""
    
    def __init__(self, ocr_language: str = 'rus+eng+ita+spa', image_quality: str = 'high'):
        self.ocr_language = ocr_language
        self.image_quality = image_quality
    
    def parse(self, file_path: str, **params) -> ParserResult:
        
        try:
            image = Image.open(file_path)
            preprocessed = self._preprocess_image(image)
            text = self._perform_ocr(preprocessed)
            
            metadata = {
                'parser': 'ImageOCRParser',
                'language': self.ocr_language,
                'image_size': image.size
            }
            
            return ParserResult(
                success=True,
                text=text,
                error_message="",
                metadata=metadata,
                file_path=file_path
            )
        except Exception as e:
            logger.error(f"OCR parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _preprocess_image(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ grayscale
        return image.convert('L')
    
    def _perform_ocr(self, image: Image.Image) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ OCR"""
        return pytesseract.image_to_string(image, lang=self.ocr_language)
    
    def get_supported_extensions(self) -> List[str]:
        return ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']


class DWGParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä DWG/DXF —Ñ–∞–π–ª–æ–≤ (—á–µ—Ä—Ç–µ–∂–∏)"""
    
    def __init__(self, extract_metadata: bool = True, extract_geometry: bool = False):
        self.extract_metadata = extract_metadata
        self.extract_geometry = extract_geometry
    
    def parse(self, file_path: str, **params) -> ParserResult:
        try:
            doc = ezdxf.readfile(file_path)
            text = self._extract_text_entities(doc)
            
            metadata = self._parse_dwg_metadata(doc) if self.extract_metadata else {}
            metadata['parser'] = 'DWGParser'
            
            return ParserResult(
                success=True,
                text=text,
                error_message="",
                metadata=metadata,
                file_path=file_path
            )
        except Exception as e:
            logger.error(f"DWG parsing error: {e}")
            return ParserResult(
                success=False,
                text="",
                error_message=str(e),
                metadata={},
                file_path=file_path
            )
    
    def _parse_dwg_metadata(self, doc) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö DWG"""
        return {
            'dxf_version': doc.dxfversion,
            'layers_count': len(doc.layers)
        }
    
    def _extract_text_entities(self, doc) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        text = ""
        msp = doc.modelspace()
        
        for entity in msp:
            if entity.dxftype() == 'TEXT':
                text += entity.dxf.text + "\n"
            elif entity.dxftype() == 'MTEXT':
                text += entity.text + "\n"
        
        return text
    
    def get_supported_extensions(self) -> List[str]:
        return ['.dwg', '.dxf']


class DepartmentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –¥–ª—è –æ—Ç–¥–µ–ª–∞"""
    
    def __init__(self, department_name: str, max_file_size: int = 100 * 1024 * 1024, timeout: int = 300):
        self.department_name = department_name
        self.allowed_parsers: Dict[str, List[Type[BaseParser]]] = {}
        self.max_file_size = max_file_size
        self.timeout = timeout
        self._parser_priorities: Dict[str, Dict[Type[BaseParser], int]] = {}
    
    def get_allowed_extensions(self) -> List[str]:
        return list(self.allowed_parsers.keys())
    
    def is_parser_allowed(self, parser_class: Type[BaseParser]) -> bool:
        for parsers in self.allowed_parsers.values():
            if parser_class in parsers:
                return True
        return False
    
    def get_parser_priority(self, extension: str, parser_class: Type[BaseParser]) -> int:
        return self._parser_priorities.get(extension, {}).get(parser_class, 999)
    
    def register_parser(self, extension: str, parser_class: Type[BaseParser], priority: int = 10):
        if extension not in self.allowed_parsers:
            self.allowed_parsers[extension] = []
        
        if parser_class not in self.allowed_parsers[extension]:
            self.allowed_parsers[extension].append(parser_class)
        
        if extension not in self._parser_priorities:
            self._parser_priorities[extension] = {}
        self._parser_priorities[extension][parser_class] = priority


class ParserRegistry:

    """–†–µ–µ—Å—Ç—Ä –ø–∞—Ä—Å–µ—Ä–æ–≤"""
    
    def __init__(self):
        self.global_registry: Dict[str, List[Type[BaseParser]]] = {}
        self.department_registries: Dict[str, DepartmentConfig] = {}
    

    def register_global_parser(self, extension: str, parser: Type[BaseParser]):
        if extension not in self.global_registry:
            self.global_registry[extension] = []
        if parser not in self.global_registry[extension]:
            self.global_registry[extension].append(parser)
    

    def register_department_parser(self, department: str, extension: str, parser: list[Type[BaseParser]]):
        if department not in self.department_registries:
            self.department_registries[department] = DepartmentConfig(department)
        
        self.department_registries[department].register_parser(extension, parser)
    

    def get_parsers_for_department(self, department: str, extension: str) -> List[Type[BaseParser]]:
        if department in self.department_registries:
            dept_config = self.department_registries[department]
            parsers = dept_config.allowed_parsers.get(extension, [])
            if parsers:
                return sorted(parsers, key=lambda p: dept_config.get_parser_priority(extension, p))
        
        return self.global_registry.get(extension, [])
    

    def get_all_supported_extensions(self) -> List[str]:
        extensions = set(self.global_registry.keys())
        for dept_config in self.department_registries.values():
            extensions.update(dept_config.get_allowed_extensions())
        return list(extensions)


class ParserManager:

    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–∞—Ä—Å–µ—Ä–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ—Ç–¥–µ–ª–æ–≤"""
    
    def __init__(self):
        self.parser_registry = ParserRegistry()
        self.parser_instances: Dict[Type[BaseParser], BaseParser] = {}
        self.file_validator = FileValidator()
        
        self._init_default_parsers()
    

    def _init_default_parsers(self):

        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""

        default_parsers = {
            'pdf': PDFParser,
            'docx': DOCParser,
            'doc': DOCParser,
            'xlsx': XLSXParser,
            'xls': XLSXParser,
            'txt': PlainTextParser,
            'dxf': DXFParser,
            'dwg': DWGParser,
            'png': ImageOCRParser,
            'jpg': ImageOCRParser,
        }

        return default_parsers
    
            
    def _parser_extension(self, file_path: str) -> str:

        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞"""

        return file_path.lower().split('.')[-1] if file_path else None
    

    def _find_parser_in_registry(self, extension: str) -> Optional[Type[BaseParser]]:

        """–ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –≤ —Ä–µ–µ—Å—Ç—Ä–µ"""

        return self._init_default_parsers().get(extension)
    

    def _save_parser_instance(self, parser_class: Type[BaseParser]):

        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ø–∞—Ä—Å–µ—Ä–∞"""

        if parser_class not in self.parser_instances:
            self.parser_instances[parser_class] = parser_class()

    def _ransfer_selected_parser(self, file_path: str, parser_class: Type[BaseParser]):

        """–ü–µ—Ä–µ–¥–∞—á–∞ —Ñ–∞–π–ª–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –ø–∞—Ä—Å–µ—Ä—É"""

        if parser_class is None:
            return None
        parser_instance = parser_class()
        return parser_instance.parse(file_path)


if __name__ == "__main__":
    path = "C:\\Users\\kulikovma\\Pictures\\Screenshots\\–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2025-09-30 162205.png"
    manager = ParserManager()
    ext = manager._parser_extension(path)
    print(ext)
    find = manager._find_parser_in_registry(ext)
    print(find)
    print(manager._ransfer_selected_parser(path, find))